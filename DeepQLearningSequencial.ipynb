{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepQLearningSequencial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWDRf4bdKeV0z16hwGX03i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF9pzKh-aBhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import gym\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr8v-tx1ovYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env= gym.make(\"MountainCar-v0\")\n",
        "low = env.observation_space.low\n",
        "high = env.observation_space.high"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd3zpL9npAMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c9313717-12bd-41c2-97e2-8129f1e34c52"
      },
      "source": [
        "print(high)\n",
        "print(low)\n",
        "print(env.action_space.n)\n",
        "print(env.observation_space.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6  0.07]\n",
            "[-1.2  -0.07]\n",
            "3\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgthE9U1pFO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae', 'mse'])\n",
        "  return model\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzxND4V7ZJh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class experience():\n",
        "  def __init__(self, buffer_size, state_dim):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.pointer = 0\n",
        "    self.state_mem = np.zeros((self.buffer_size, *state_dim), dtype=np.int32)\n",
        "    self.action_mem = np.zeros(self.buffer_size, dtype=np.int32)\n",
        "    self.next_state_mem = np.zeros((self.buffer_size, *state_dim), dtype=np.int32)\n",
        "    self.reward_mem = np.zeros(self.buffer_size, dtype=np.int32)\n",
        "    self.done_mem = np.zeros(self.buffer_size, dtype=np.int32)\n",
        "\n",
        "  def add_exp(self, state, action, reward, next_state, done):\n",
        "    idx = self.pointer % self.buffer_size\n",
        "    self.state_mem[idx] = state\n",
        "    self.action_mem[idx] = action\n",
        "    self.reward_mem[idx] = reward\n",
        "    self.next_state_mem[idx] = next_state\n",
        "    self.done_mem[idx] =  1 + int(done)\n",
        "    self.pointer += 1\n",
        "\n",
        "  def sample_exp(self, batch_size):\n",
        "    max_mem = min(self.pointer, self.buffer_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace= False)\n",
        "    state = self.state_mem[batch]\n",
        "    action = self.action_mem[batch]\n",
        "    reward = self.reward_mem[batch]\n",
        "    next_state = self.next_state_mem[batch]\n",
        "    done = self.done_mem[batch]\n",
        "    return state, action , reward, next_state, done\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXTnZdGXigsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class agent():\n",
        "  def __init__(self):\n",
        "    self.q_net = model()\n",
        "    self.target_net = model()\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_decay = 1e-3\n",
        "    self.min_epsilon = 0.01\n",
        "    self.memory = experience(buffer_size=1000000, state_dim=env.observation_space.shape)\n",
        "    self.batch_size = 64\n",
        "    self.gamma = 0.99\n",
        "\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      self.update_epsilon()\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    else:\n",
        "      state = np.array([state])\n",
        "      action = self.q_net.predict(state)\n",
        "      action = np.argmax(action)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "  def update_mem(self, state, action, reward, next_state, done):\n",
        "    self.memory.add_exp(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_net.set_weights(self.q_net.get_weights())  \n",
        "\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
        "    return self.epsilon\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    if self.memory.pointer < self.batch_size:\n",
        "      return \n",
        "    state, action, reward, next_state, done = self.memory.sample_exp(self.batch_size)\n",
        "    target = self.q_net.predict(state)\n",
        "    next_state_val = self.target_net.predict(next_state)\n",
        "    batch_index = np.arange(self.batch_size)\n",
        "    target[batch_index, action] = reward + self.gamma * np.amax(next_state_val, axis=1)*done\n",
        "    self.q_net.train_on_batch(state, target)\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        self.q_net.save(\"model.h5\")\n",
        "        self.target_net.save(\"target_model.h5\")\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_net = load_model(\"model.h5\")\n",
        "        self.target_net = load_model(\"model.h5\")\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DC5ilKyVu59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cfa7604b-9e5c-4276-e521-44b81a2fb577"
      },
      "source": [
        "agentoo7 = agent()\n",
        "agentoo7.q_net.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 64)                192       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 4,547\n",
            "Trainable params: 4,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PvqKKEagO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agentoo7.target_net.set_weights(agentoo7.q_net.get_weights())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykDQGVw5-4vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.compat.v1.disable_eager_execution()\n",
        "steps = 5000\n",
        "for s in range(steps):\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  t = 0\n",
        "  while not done:\n",
        "    #env.render()\n",
        "    action = agentoo7.act(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    agentoo7.update_mem(state, action, reward, next_state, done)\n",
        "    if s % 10 == 0 and s != 0:\n",
        "      agentoo7.update_target()\n",
        "    agentoo7.train()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    t += 1\n",
        "    if done:\n",
        "      print(\"total reward after {} steps is {}\".format(s, total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Wxwd69DFbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
